 Variational Bayes methods have emerged as a fast and computationally efficient alternative to Markov chain Monte Carlo methods for scalable Bayesian estimation of mixed multinomial logit models . It has been established that VB is substantially faster than MCMC at practically no compromises in predictive accuracy . In this paper we address two critical gaps concerning the usage and understanding of VB for MMNL . First extant VB methods are limited to utility specifications involving only individual specific taste parameters . Second the finite sample properties of VB estimators and the relative performance of VB MCMC and maximum simulated likelihood estimation are not known . To address the former this study extends several VB methods for MMNL to admit utility specifications including both fixed and random utility parameters . To address the latter we conduct an extensive simulation based evaluation to benchmark the extended VB methods against MCMC and MSLE in terms of estimation times parameter recovery and predictive accuracy . The results suggest that all VB variants with the exception of the ones relying on an alternative variational lower bound constructed with the help of the modified Jensens inequality perform as well as MCMC and MSLE at prediction and parameter recovery . In particular VB with nonconjugate variational message passing and the delta method is up to 16 times faster than MCMC and MSLE . Thus VB NCVMP can be an attractive alternative to MCMC and MSLE for fast scalable and accurate estimation of MMNL models .

@highlight Address gaps in use of Variational Bayes VB for Mixed Multinomial Logit MMNL .
@highlight Extend VB methods for MMNL to admit utility with fixed and random parameters.
@highlight Compare VB with Gibbs sampler classical estimator of MMNL in Monte Carlo study.
@highlight VB variant is as good as Gibbs sampler classical estimator at parameter recovery.
@highlight One VB variant is up to 16 times faster than Gibbs sampler classical estimator.
