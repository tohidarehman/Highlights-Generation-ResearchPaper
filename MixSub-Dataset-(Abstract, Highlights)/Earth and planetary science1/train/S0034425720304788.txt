 Land cover mapping in complex environments can be challenging due to their landscape heterogeneity . With the increasing availability of various open access remotely sensed datasets more images acquired by different sensors and on different dates tend to be used to improve land cover classification accuracy . Selecting an appropriate feature domain with the best landscape separability is therefore crucial in meeting the requirement of computational efficiency and model interpretability . Variable selection is widely used in pattern recognition to enhance model parsimony . This study focused on the variable selection process and proposed a series of methods to select the optimal feature domain to improve land cover classification in a complex urbanized coastal area . Two decision tree models and five variable importance measures based on random forests were considered . Variable importance measures were applied to a set of spectral spatial and temporal features derived from medium resolution satellite images . Backward elimination methods were used to select the optimal feature subset . It is found that compared to the traditional band only model the variable selection process can significantly improve the model parsimony and computational efficiency . The CPVIM based on CIT decision tree model was more reliable in selecting relevant features regardless their correlations but CART tended to generate higher classification accuracy . Therefore the combination of the CART model and the ranking from the CPVIM variable measure is recommended to achieve higher classification accuracy and better data interpretability . The novelty of our work is with the insight into the merits of integrating variable selection in the land cover classification process over complex environments .

@highlight Variable selection can significantly improve coastal land cover classification.
@highlight The selection of variable importance measures may vary by data types.
@highlight Conditional permutated variable importance measure was reliable for correlated data.
@highlight Conditional Inference Tree took more time but did not necessarily improve accuracy.
