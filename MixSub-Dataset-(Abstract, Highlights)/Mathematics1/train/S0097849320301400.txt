 Scene understanding of large scale 3D point clouds of an outer space is still a challenging task . Compared with simulated 3D point clouds the raw data from LiDAR scanners consist of tremendous points returned from all possible reflective objects and they are usually non uniformly distributed . Therefore its cost effective to develop a solution for learning from raw large scale 3D point clouds . In this track we provide large scale 3D point clouds of street scenes for the semantic segmentation task . The data set consists of 80 samples with 60 for training and 20 for testing . Each sample with over 2 million points represents a street scene and includes a couple of objects . There are five meaningful classes building car ground pole and vegetation . We aim at localizing and segmenting semantic objects from these large scale 3D point clouds . Four groups contributed their results with different methods . The results show that learning based methods are the trend and one of them achieves the best performance on both Overall Accuracy and mean Intersection over Union . Next to the learning based methods the combination of hand crafted detectors are also reliable and rank second among comparison algorithms .

@highlight Provide a large scale 3D street scene point cloud dataset for 3D semantic segmentation.
@highlight Evaluate different algorithms on the dataset and help finding solutions for large scale 3D point cloud processing. We have five algorithms under evaluation with one based on handcrafted detectors one based on 3D to 2D projection learning and the other three being end to end learning based methods.
@highlight The results show that point set based end to end learning methods can learn representative features directly from 3D points and performs better than handcrafted methods.
