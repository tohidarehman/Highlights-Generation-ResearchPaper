 Random Balance strategy has been recently proposed for constructing classifier ensembles for imbalanced two class data sets . In RandBal each base classifier is trained with a sample of the data with a random class prevalence independent of the a priori distribution . Hence for each sample one of the classes will be undersampled while the other will be oversampled . RandBal can be applied on its own or can be combined with any other ensemble method . One particularly successful variant is RandBalBoost which integrates Random Balance and boosting . Encouraged by the success of RandBal this work proposes two approaches which extend RandBal to multiclass imbalance problems . Multiclass imbalance implies that at least two classes have substantially different proportion of instances . In the first approach proposed here termed Multiple Random Balance we deal with all classes simultaneously . The training data for each base classifier are sampled with random class proportions . The second approach we propose decomposes the multiclass problem into two class problems using one vs one or one vs all and builds an ensemble of RandBal ensembles . We call the two versions of the second approach OVO RandBal and OVA RandBal respectively . These two approaches were chosen because they are the most straightforward extensions of RandBal for multiple classes . Our main objective is to evaluate both approaches for multiclass imbalanced problems . To this end an experiment was carried out with 52 multiclass data sets . The results suggest that both MultiRandBal and OVO OVA RandBal are viable extensions of the original two class RandBal . Collectively they consistently outperform acclaimed state of the art methods for multiclass imbalanced problems .

@highlight Random Balance ensembles are extended from binary to multiple classes.
@highlight In the first approach the proportions of the multiple are assigned randomly.
@highlight The second approach is based on binarization techniques one vs one or one vs all .
@highlight Random Balance ensembles are also combined with Bagging and Boosting.
@highlight The experiments over 52 data sets show the viability of both approaches.
