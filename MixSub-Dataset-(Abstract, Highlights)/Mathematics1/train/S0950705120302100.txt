 In this work we reveal an essential problem rarely discussed in current semi supervised learning literatures the learned feature distribution mismatch problem between labeled samples and unlabeled samples . It is common knowledge that learning from the limited labeled data easily leads to overfitting . However the difference between the inferred labels of unlabeled data and the ground truths of labeled data may make the learned features of labeled and unlabeled data have different distributions . This distribution mismatch problem may destroy the assumption of smoothness widely used in semi supervised field resulting in unsatisfactory performance . In this paper we propose a novel Semi supervised Dual Branch Network in which the first branch is trained with labeled and unlabeled data and the other is trained with the predictions of unlabeled data generated from the first branch only . To avoid the different distributions between ground truth labels and inferred labels for the unlabeled data we proposed an effective co consistency loss to overcome the mismatch problem and a mix consistency loss to make each branch learn a consistent feature representation . Meanwhile we designed an augmentation supervised loss for the first branch to further alleviate the mismatch problem . With the designed three kinds of losses the proposed SDB Net can be efficiently trained . The experimental results on three benchmark datasets such as CIFAR 10 CIFAR 100 and SVHN show the superior performance of the proposed SDB Net .

@highlight We present a rarely discussed learned feature distribution mismatch issue between labeled data and unlabeled data.
@highlight We design a dual branch architecture and a co consistency regularization to explicitly address mismatch issue.
@highlight We design an augmentation supervised loss for preventing overfitting.
@highlight The results of our method outperform the related works and the state of the arts.
