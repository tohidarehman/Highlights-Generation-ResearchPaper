 In Deep Learning training a model properly with a high quantity and quality of data is crucial in order to achieve a good performance . In some tasks however the necessary data is not available at a particular moment and only becomes available over time . In which case incremental learning is used to train the model correctly . An open problem remains however in the form of the stabilityplasticity dilemma how to incrementally train a model that is able to respond well to new data while also retaining previous knowledge . In this paper an incremental learning model inspired in Rehearsal named CRIF is proposed and two instances for the framework are employed one using a random based selection of representative samples the other using Crowding Distance and Best vs. Second Best metrics in conjunction for this task . The experiments were performed on five datasets MNIST Fashion MNIST CIFAR 10 Caltech 101 and Tiny ImageNet in two different incremental scenarios a strictly class incremental scenario and a pseudo class incremental scenario with unbalanced data . In Caltech 101 Transfer Learning was used and in this scenario as well as in the other three datasets the proposed method NIL achieved better results in most of the quality metrics than comparison algorithms such as RMSProp Inc and iCaRL and outperformed the other proposed method RILBC . NIL also requires less time to achieve these results .

@highlight An incremental learning model inspired in Rehearsal recall of past memories based on a subset of data is proposed.
@highlight Experiments were performed over MNIST Fashion MNIST CIFAR 10 and Caltech 101 in two different scenarios.
@highlight Several metrics were used to compare learning quality results when each new megabatch of data is used.
@highlight Friedmans non parametric statistical test and Holm post hoc test were used for supporting the analysis of the results.
@highlight Random based selection of representative samples obtains the best results.
