 In this paper, we present a multi-modal perception based framework to realize a non-intrusive domestic assistive robotic system. It is non-intrusive in that it only starts interaction with a user when it detects the user’s intention to do so. All the robot’s actions are based on multi-modal perceptions which include user detection based on RGB-D data, user’s intention-for-interaction detection with RGB-D and audio data, and communication via user distance mediated speech recognition. The utilization of multi-modal cues in different parts of the robotic activity paves the way to successful robotic runs (94% success rate). Each presented perceptual component is systematically evaluated using appropriate dataset and evaluation metrics. Finally the complete system is fully integrated on the PR2 robotic platform and validated through system sanity check runs and user studies with the help of 17 volunteer elderly participants.

@highlight We present a complete multi-modal perception driven non-intrusive domestic robotic system for the elderly.
@highlight We present a novel multi-modal user’s intention-for-interaction detection modality.
@highlight A fusion method to improve the speech recognition given the user’s position, available sensors, and recognition tools is presented.
@highlight We present details of the complete implemented system along with relevant evaluations that demonstrate the soundness of the framework via an exemplar application whereby the robot helps the user find hidden or misplaced objects in his/her living place.
@highlight The proposed framework is further investigated by conducting relevant user studies involving 17 elderly participants.
