 We consider the problem of multi-label classification where a feature vector may belong to one of more different classes or concepts at the same time. Many existing approaches are devoted for solving the difficult estimation task of uncovering the relationship between features and active concepts, solely from data without taking into account any sensible functional structure. In this paper, we propose a novel probabilistic generative model that aims to describe the core generative process of how multiple active concepts can contribute to feature generation. Within our model, each concept is associated with multiple representative base feature vectors, which shares the central idea of sparse feature modeling with the popular dictionary learning. However, by dealing with the weight coefficients as exclusive latent random variables encoding contribution levels, we effectively frame the coefficient learning task as probabilistic inference. We introduce two parameter learning algorithms for the proposed model: one based on standard maximum likelihood learning via the expectationâ€“maximization algorithm, the other focusing on maximally separating the margin of the true concept configuration away from the class boundary. In the latter we suggest an efficient approximate optimization method where each iteration admits closed-form update with no line search. For several benchmark datasets mostly from the multi-label image classification, we demonstrate that our generative model with proposed estimators can often yield superior prediction performance to existing methods.

@highlight Novel generative model representing fusion of multiple concepts in feature generation.
@highlight Learning algorithms based on likelihood and margin maximization without line search.
@highlight Performance improvement in several multi-label image classification tasks.
