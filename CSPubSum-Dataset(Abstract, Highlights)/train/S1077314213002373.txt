 In this paper we address an important issue in human–robot interaction, that of accurately deriving pointing information from a corresponding gesture. Based on the fact that in most applications it is the pointed object rather than the actual pointing direction which is important, we formulate a novel approach which takes into account prior information about the location of possible pointed targets. To decide about the pointed object, the proposed approach uses the Dempster–Shafer theory of evidence to fuse information from two different input streams: head pose, estimated by visually tracking the off-plane rotations of the face, and hand pointing orientation. Detailed experimental results are presented that validate the effectiveness of the method in realistic application setups.

@highlight Problem formulation: given a number of possible pointed targets, compute the target that the user points to.
@highlight Estimate head pose by visually tracking the off-plane rotations of the face.
@highlight Recognize two different hand pointing gestures (point left and point right).
@highlight Model the problem using the Dempster–Shafer theory of evidence.
@highlight Use Demspster’s rule of combination to fuse information and derive the pointed target.
