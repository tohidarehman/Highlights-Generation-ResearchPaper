 The Health Insurance Portability and Accountability Act (HIPAA) Safe Harbor method requires removal of 18 types of protected health information (PHI) from clinical documents to be considered “de-identified” prior to use for research purposes. Human review of PHI elements from a large corpus of clinical documents can be tedious and error-prone. Indeed, multiple annotators may be required to consistently redact information that represents each PHI class. Automated de-identification has the potential to improve annotation quality and reduce annotation time. For instance, using machine-assisted annotation by combining de-identification system outputs used as pre-annotations and an interactive annotation interface to provide annotators with PHI annotations for “curation” rather than manual annotation from “scratch” on raw clinical documents. In order to assess whether machine-assisted annotation improves the reliability and accuracy of the reference standard quality and reduces annotation effort, we conducted an annotation experiment. In this annotation study, we assessed the generalizability of the VA Consortium for Healthcare Informatics Research (CHIR) annotation schema and guidelines applied to a corpus of publicly available clinical documents called MTSamples. Specifically, our goals were to  characterize a heterogeneous corpus of clinical documents manually annotated for risk-ranked PHI and other annotation types (clinical eponyms and person relations),  evaluate how well annotators apply the CHIR schema to the heterogeneous corpus,  compare whether machine-assisted annotation (experiment) improves annotation quality and reduces annotation time compared to manual annotation (control), and  assess the change in quality of reference standard coverage with each added annotator’s annotations.

@highlight MTSamples was rich in Health Care Provider Names, Dates, and Health care Unit Names.
@highlight Recall, precision, F1-measure (control 0.84, 0.94, 0.89, experiment 0.84, 0.85, 0.84).
@highlight Annotating raw clinical text generated the highest quality data.
@highlight No significant time-savings were observed using machine-assisted annotation.
@highlight With added annotators recall increased 0.66–0.92, precision decreased 0.82–0.61.
