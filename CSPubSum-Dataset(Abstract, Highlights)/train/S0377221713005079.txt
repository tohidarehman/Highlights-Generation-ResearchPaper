 We consider finite horizon Markov decision processes under performance measures that involve both the mean and the variance of the cumulative reward. We show that either randomized or history-based policies can improve performance. We prove that the complexity of computing a policy that maximizes the mean reward under a variance constraint is NP-hard for some cases, and strongly NP-hard for others. We finally offer pseudopolynomial exact and approximation algorithms.

@highlight The mean and variance of the total reward in Markov decision processes are studied.
@highlight Randomized or history-based policies can improve performance for these criteria.
@highlight Computing an optimal policy under a variance constraint is shown to be NP-hard.
@highlight Pseudopolynomial exact and approximation algorithms are proposed.
