 In this article we investigate what representations of acoustics and word usage are most suitable for predicting dimensions of affect—arousal, valance, power and expectancy—in spontaneous interactions. Our experiments are based on the AVEC 2012 challenge dataset. For lexical representations, we compare corpus-independent features based on psychological word norms of emotional dimensions, as well as corpus-dependent representations. We find that corpus-dependent bag of words approach with mutual information between word and emotion dimensions is by far the best representation. For the analysis of acoustics, we zero in on the question of granularity. We confirm on our corpus that utterance-level features are more predictive than word-level features. Further, we study more detailed representations in which the utterance is divided into regions of interest (ROI), each with separate representation. We introduce two ROI representations, which significantly outperform less informed approaches. In addition we show that acoustic models of emotion can be improved considerably by taking into account annotator agreement and training the model on smaller but reliable dataset. Finally we discuss the potential for improving prediction by combining the lexical and acoustic modalities. Simple fusion methods do not lead to consistent improvements over lexical classifiers alone but improve over acoustic models.

@highlight Study a variety of representations of lexical usage and acoustics for continuous affect recognition in spontaneous speech.
@highlight We perform acoustic analysis on different regions of interest (ROI) and show substantial improvements.
@highlight Lexical feature with sparse representation using mutual information between words and dimension as weights is powerful.
@highlight Our proposed acoustic and lexical representations are complementary and show potential improvement for fusion.
@highlight On the AVEC 2012, our approach outperforms other participants and baselines by a large margin.
