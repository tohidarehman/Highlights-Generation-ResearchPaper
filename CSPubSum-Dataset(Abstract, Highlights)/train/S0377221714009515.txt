 Many practical optimization problems are dynamically changing, and require a tracking of the global optimum over time. However, tracking usually has to be quick, which excludes re-optimization from scratch every time the problem changes. Instead, it is important to make good use of the history of the search even after the environment has changed. In this paper, we consider Efficient Global Optimization (EGO), a global search algorithm that is known to work well for expensive black box optimization problems where only few function evaluations are possible. It uses metamodels of the objective function for deciding where to sample next. We propose and compare four methods of incorporating old and recent information in the metamodels of EGO in order to accelerate the search for the global optima of a noise-free objective function stochastically changing over time. As we demonstrate, exploiting old information as much as possible significantly improves the tracking behavior of the algorithm.

@highlight Metamodel-based optimization for expensive dynamic black box functions.
@highlight Novel adaptation of efficient global optimization to dynamic environments.
@highlight Four approaches to decrease reliance on old information empirically compared.
@highlight Comparisons with naive approaches of re-optimization or ignoring change show significant improvement.
