 Background and significance Sparsity is often a desirable property of statistical models, and various feature selection methods exist so as to yield sparser and interpretable models. However, their application to biomedical text classification, particularly to mortality risk stratification among intensive care unit (ICU) patients, has not been thoroughly studied. Objective To develop and characterize sparse classifiers based on the free text of nursing notes in order to predict ICU mortality risk and to discover text features most strongly associated with mortality. Methods We selected nursing notes from the first 24h of ICU admission for 25,826 adult ICU patients from the MIMIC-II database. We then developed a pair of stochastic gradient descent-based classifiers with elastic-net regularization. We also studied the performance-sparsity tradeoffs of both classifiers as their regularization parameters were varied. Results The best-performing classifier achieved a 10-fold cross-validated AUC of 0.897 under the log loss function and full L 2 regularization, while full L 1 regularization used just 0.00025% of candidate input features and resulted in an AUC of 0.889. Using the log loss (range of AUCs 0.889–0.897) yielded better performance compared to the hinge loss (0.850–0.876), but the latter yielded even sparser models. Discussion Most features selected by both classifiers appear clinically relevant and correspond to predictors already present in existing ICU mortality models. The sparser classifiers were also able to discover a number of informative – albeit nonclinical – features. Conclusion The elastic-net-regularized classifiers perform reasonably well and are capable of reducing the number of features required by over a thousandfold, with only a modest impact on performance.

@highlight To date, ICU mortality prediction has relied on structured clinical features.
@highlight However, clinical free text features seem to perform just as well, or better.
@highlight Existing clinical NLP methods largely do not rely on feature selection techniques.
@highlight Applying regularization can aid discovery of important features in these problems.
@highlight Indeed, only a small fraction of all features are needed to predict mortality well.
