 This article explores whether people more frequently attempt to repair misunderstandings when speaking to an artificial conversational agent if it is represented as fully human. Interactants in dyadic conversations with an agent (the chat bot Cleverbot) spoke to either a text screen interface (agent's responses shown on a screen) or a human body interface (agent's responses vocalized by a human speech shadower via the echoborg method) and were either informed or not informed prior to interlocution that their interlocutor's responses would be agent-generated. Results show that an interactant is less likely to initiate repairs when an agent-interlocutor communicates via a text screen interface as well as when they explicitly know their interlocutor's words to be agent-generated. That is to say, people demonstrate the most “intersubjective effort” toward establishing common ground when they engage an agent under the same social psychological conditions as face-to-face human–human interaction (i.e., when they both encounter another human body and assume that they are speaking to an autonomously-communicating person). This article's methodology presents a novel means of benchmarking intersubjectivity and intersubjective effort in human-agent interaction. “Intersubjectivity has [ … ] to be taken for granted in order to be achieved.” – Rommetveit (1974, p. 56)

@highlight This article demonstrates a method for evaluating human-agent interaction against human–human benchmarks.
@highlight An experiment assesses the effort people exert toward building common ground with a conversational agent.
@highlight Believing an interlocutor is a person (vs. an agent) augments efforts to establish common ground.
@highlight Interfacing with a human body (vs. a text screen) augments efforts to establish common ground.
