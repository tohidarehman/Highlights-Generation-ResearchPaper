 Object tracking quality usually depends on video scene conditions (e.g. illumination, density of objects, object occlusion level). In order to overcome this limitation, this article presents a new control approach to adapt the object tracking process to the scene condition variations. More precisely, this approach learns how to tune the tracker parameters to cope with the tracking context variations. The tracking context, or context, of a video sequence is defined as a set of six features: density of mobile objects, their occlusion level, their contrast with regard to the surrounding background, their contrast variance, their 2D area and their 2D area variance. In an offline phase, training video sequences are classified by clustering their contextual features. Each context cluster is then associated to satisfactory tracking parameters. In the online control phase, once a context change is detected, the tracking parameters are tuned using the learned values. The approach has been experimented with three different tracking algorithms and on long, complex video datasets. This article brings two significant contributions:  a classification method of video sequences to learn offline tracking parameters and  a new method to tune online tracking parameters using tracking context.

@highlight We present a new control approach to adapt trackers to scene condition variations.
@highlight Tracking context is defined as six features describing scene condition.
@highlight Best tracker parameters are learned offline for tracking contexts.
@highlight Trackers are then controlled by tuning online their parameters.
@highlight Experimental results are compared with several recent state of the art trackers.
